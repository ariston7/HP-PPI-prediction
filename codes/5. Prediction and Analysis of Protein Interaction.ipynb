{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Protein Interaction Network\n",
    "\n",
    "Machine learning classification task for predicting whether a protein pair interacts or not\n",
    "\n",
    "Protein pairs are generated from a test pathogen against human proteins contained in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import itertools\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from Bio import SearchIO\n",
    "\n",
    "from features import domain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "dir_in = dir_out = os.path.join(parent_dir, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain candidate protein domains\n",
    "\n",
    "Protein sequence source: https://www.uniprot.org\n",
    "\n",
    "Pathogen: *Streptococcus pneumoniae* strain D39 (**STRP2**; Taxonomy ID 373153)\n",
    ">Keyword filters\n",
    "- `Organism [OS]: Streptococcus pneumoniae serotype 2 (strain D39 / NCTC 7466) [373153]`\n",
    "- `Sequence length: From 50`\n",
    "- `Gene Ontology [GO]: extracellular region [5576] (20), pathogenesis [9405] (3)`\n",
    "- `All: choline (9), sialidase (2), amidase (4), virulence (4), surface protein (2), adhesion (3)`\n",
    "- Total: 42 proteins\n",
    "\n",
    "Host: *Homo sapiens* (Taxonomy ID 9606)\n",
    ">Keyword filters\n",
    "- `Organism [OS]: Homo sapiens (Human) [9606]`\n",
    "- `Proteome ID: up000005640`\n",
    "- `Sequence length: From 50`\n",
    "- `All: extracellular`\n",
    "- `Gene Ontology [GO]: immunoglobulin receptor activity [19763] (6), regulation of complement activation [30449] (120), phagocytosis [6909] (204), toll-like receptor signalling pathway [2224] (54), plasminogen activation [31639] (15), defense response to Gram-positive bacterium [50830] (87)`\n",
    "- Total: 395 proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Extraction of Pfam domains: `hmmscan`\n",
    "- `hmmscan --tblout STRP2_pfam_hits --acc --noali -E 0.00001 --domE 0.00001 --cpu 7 ~/hmmer-3.2.1/pfam/Pfam-A.hmm STRP2_sequences.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 42 STRP2 proteins for HP-PPI prediction\n",
      "Selected 388 HUMAN proteins for HP-PPI prediction\n"
     ]
    }
   ],
   "source": [
    "# Parse hmmscan result of STRP2\n",
    "pfam_set = joblib.load('pfam.pkl')[1] # Pfam domains available in the training dataset\n",
    "prot_lists = {}\n",
    "pfam_dict = {} # generate a new dict for the candidate proteins\n",
    "\n",
    "for organism in ['STRP2', 'HUMAN']:\n",
    "    f_in = os.path.join(dir_in, 'prediction', '%s_pfam_hits' % organism)\n",
    "    proteins = [] # store Uniprot accessions of the current organism\n",
    "    \n",
    "    for query in SearchIO.parse(f_in, 'hmmer3-tab'):\n",
    "        uniprot_id = query.id.split('|')[2] # use protein name to ease network analysis\n",
    "        domain_counts = {}\n",
    "\n",
    "        # Read each domain hits in query\n",
    "        for hit in query.hits:\n",
    "            pfam_acc = hit.accession.split('.')[0] # Pfam accession of domain\n",
    "\n",
    "            # Select only domains that exists in the training set\n",
    "            if pfam_acc in pfam_set: \n",
    "                domain_counts[pfam_acc] = hit.domain_reported_num\n",
    "\n",
    "        # Add the pathogen protein to an existing domains dict\n",
    "        pfam_dict[uniprot_id] = domain_counts\n",
    "        proteins.append(uniprot_id)\n",
    "    \n",
    "    # Store protein list of the current organism in a dict\n",
    "    prot_lists[organism] = proteins\n",
    "    \n",
    "    # Print statistics\n",
    "    print('Selected %i %s proteins for HP-PPI prediction' % (len(proteins), organism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate candidate protein pairs and extract domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16296 protein pairs for prediction\n"
     ]
    }
   ],
   "source": [
    "# Generate protein pairs\n",
    "pairs = [pair for pair in itertools.product(prot_lists['STRP2'], prot_lists['HUMAN'])]\n",
    "print('Generated %i protein pairs for prediction' % len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:00<00:00, 980.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up feature extraction function\n",
    "feature_function = partial(domain_features,\n",
    "                           domain_dict=pfam_dict,\n",
    "                           domain_set=pfam_set)\n",
    "\n",
    "# Get features of each protein as dict to speed up\n",
    "# extraction from pairs\n",
    "all_prots = prot_lists['STRP2'] + prot_lists['HUMAN']\n",
    "feat_dict = {prot: feature_function(prot) for prot in tqdm(all_prots)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP-PPI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('scaler', MaxAbsScaler(copy=True)),\n",
       "  ('clf',\n",
       "   LogisticRegression(C=2.7421483541202414, class_weight=None, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                      max_iter=1000, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                      random_state=149028763, solver='liblinear', tol=0.0001,\n",
       "                      verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'scaler': MaxAbsScaler(copy=True),\n",
       " 'clf': LogisticRegression(C=2.7421483541202414, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=1000, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=149028763, solver='liblinear', tol=0.0001,\n",
       "                    verbose=0, warm_start=False),\n",
       " 'scaler__copy': True,\n",
       " 'clf__C': 2.7421483541202414,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__dual': False,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__l1_ratio': None,\n",
       " 'clf__max_iter': 1000,\n",
       " 'clf__multi_class': 'warn',\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__penalty': 'l1',\n",
       " 'clf__random_state': 149028763,\n",
       " 'clf__solver': 'liblinear',\n",
       " 'clf__tol': 0.0001,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained classifier\n",
    "clf = joblib.load('best_model.pkl')\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eigenvector_centrality</th>\n",
       "      <th>Degree_centrality</th>\n",
       "      <th>Clustering_coefficient</th>\n",
       "      <th>Betweenness_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P00352</th>\n",
       "      <td>2.207378e-32</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B4E3U0,Q13683,Q4LE35</th>\n",
       "      <td>1.048165e-03</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P02708,Q53SH4</th>\n",
       "      <td>1.204406e-04</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q9ULJ8,A1L494,B7ZLX4</th>\n",
       "      <td>1.922150e-03</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P63261</th>\n",
       "      <td>6.808116e-03</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.002309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Eigenvector_centrality  Degree_centrality  \\\n",
       "P00352                          2.207378e-32           0.000209   \n",
       "B4E3U0,Q13683,Q4LE35            1.048165e-03           0.000626   \n",
       "P02708,Q53SH4                   1.204406e-04           0.000417   \n",
       "Q9ULJ8,A1L494,B7ZLX4            1.922150e-03           0.000939   \n",
       "P63261                          6.808116e-03           0.004381   \n",
       "\n",
       "                      Clustering_coefficient  Betweenness_centrality  \n",
       "P00352                              0.000000                0.000000  \n",
       "B4E3U0,Q13683,Q4LE35                0.166667                0.000298  \n",
       "P02708,Q53SH4                       0.000000                0.000401  \n",
       "Q9ULJ8,A1L494,B7ZLX4                0.238095                0.000038  \n",
       "P63261                              0.025641                0.002309  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load human PPI graph topological properties as DataFrame\n",
    "f_in = os.path.join(dir_in, 'human_ppi_topology.tsv')\n",
    "\n",
    "df_props = pd.read_csv(f_in, sep='\\t', index_col=0)\n",
    "df_props.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16296/16296 [01:16<00:00, 212.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prediction task\n",
    "X = []\n",
    "for pair in tqdm(pairs):\n",
    "    domain_features = sum(map(feat_dict.get, pair))\n",
    "    graph_features = df_props[df_props.index.str.contains(pair[1])].values\n",
    "\n",
    "    # Combine features\n",
    "    features = sparse.hstack((domain_features,\n",
    "                              np.resize(graph_features, (1,4))))\n",
    "    X.append(features)\n",
    "\n",
    "X = sparse.vstack(X)\n",
    "\n",
    "# Predict labels\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 9835 interactions (60.35% of candidate pairs)\n"
     ]
    }
   ],
   "source": [
    "# Examine prediction result\n",
    "n_ppi = predictions.sum() # number of interactions reported\n",
    "print('Predicted %i interactions (%.2f%% of candidate pairs)' % (n_ppi, (n_ppi/len(pairs) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pathogen_Protein</th>\n",
       "      <th>Human_Protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENO_STRP2</td>\n",
       "      <td>FCAR_HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENO_STRP2</td>\n",
       "      <td>FCG2B_HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENO_STRP2</td>\n",
       "      <td>FCERG_HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENO_STRP2</td>\n",
       "      <td>FCGRB_HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENO_STRP2</td>\n",
       "      <td>CLUS_HUMAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Pathogen_Protein Human_Protein\n",
       "0        ENO_STRP2    FCAR_HUMAN\n",
       "1        ENO_STRP2   FCG2B_HUMAN\n",
       "2        ENO_STRP2   FCERG_HUMAN\n",
       "3        ENO_STRP2   FCGRB_HUMAN\n",
       "4        ENO_STRP2    CLUS_HUMAN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain interacting pairs\n",
    "interactions = np.array(pairs)[predictions.astype(bool)]\n",
    "\n",
    "# Save interactions as DataFrame\n",
    "f_out = os.path.join(dir_out, 'prediction', 'predicted_interactions.tsv')\n",
    "\n",
    "df = pd.DataFrame(interactions, columns=['Pathogen_Protein', 'Human_Protein'])\n",
    "df.to_csv(f_out, sep='\\t', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
