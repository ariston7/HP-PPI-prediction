{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Protein Interaction Network\n",
    "\n",
    "Machine learning classification task for predicting whether a protein pair interacts or not\n",
    "\n",
    "Protein pairs are generated from a test pathogen against human proteins contained in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import itertools\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from Bio import SearchIO\n",
    "\n",
    "from features import domain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "dir_in = os.path.join(parent_dir, 'data')\n",
    "dir_out = os.path.join(parent_dir, 'data', 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain pathogen protein domains\n",
    "\n",
    "Pathogen: *Streptococcus pneumoniae* strain D39 (**STRP2**)\n",
    "\n",
    "Protein sequence source: https://www.uniprot.org\n",
    "\n",
    "- `Organism [OS]: Streptococcus pneumoniae serotype 2 (strain D39 / NCTC 7466) [373153]`\n",
    "- `Sequence length: From 50`\n",
    "\n",
    "Extraction of Pfam domains: `hmmscan`\n",
    "- `hmmscan --tblout STRP2_pfam_hits --acc --noali -E 0.00001 --domE 0.00001 --cpu 7 ~/hmmer-3.2.1/pfam/Pfam-A.hmm STRP2_sequences.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pfam data from the training pathogens\n",
    "pfam_dict, pfam_set = joblib.load('pfam.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1643 STRP2 proteins for HP-PPI prediction\n"
     ]
    }
   ],
   "source": [
    "# Parse hmmscan result of STRP2\n",
    "f_in = os.path.join(dir_in, 'STRP2_pfam_hits')\n",
    "pathogen_prots = [] # store Uniprot accessions of STRP2\n",
    "\n",
    "for query in SearchIO.parse(f_in, 'hmmer3-tab'):\n",
    "    uniprot_id = query.id.split('|')[1]\n",
    "    domains = []\n",
    "    \n",
    "    # Read each domain hits in query\n",
    "    for hit in query.hits:\n",
    "        pfam_acc = hit.accession.split('.')[0] # Pfam accession of domain\n",
    "        \n",
    "        # Select only domains that exists in the training set\n",
    "        if pfam_acc in pfam_set: \n",
    "            domains.append(pfam_acc)\n",
    "        \n",
    "    # Add the pathogen protein to an existing domains dict\n",
    "    pfam_dict[uniprot_id] = domains \n",
    "    pathogen_prots.append(uniprot_id)\n",
    "\n",
    "# Print statistics\n",
    "print('Selected %i STRP2 proteins for HP-PPI prediction' % len(pathogen_prots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate candidate protein pairs and extract domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 3188 human proteins\n"
     ]
    }
   ],
   "source": [
    "# Extract human proteins from training dataset\n",
    "f_in = os.path.join(dir_in, 'positive_pairs.tsv')\n",
    "\n",
    "human_prots = set(pd.read_csv(f_in, sep='\\t')['Human_Uniprot_ID'])\n",
    "print('Obtained %i human proteins' % len(human_prots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5237884 protein pairs for prediction\n"
     ]
    }
   ],
   "source": [
    "# Generate protein pairs\n",
    "pairs = [pair for pair in itertools.product(pathogen_prots, human_prots)]\n",
    "print('Generated %i protein pairs for prediction' % len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to aid parallelization\n",
    "feature_function = partial(domain_features,\n",
    "                           domain_dict=pfam_dict,\n",
    "                           domain_set=pfam_set)\n",
    "\n",
    "def get_features(pair):\n",
    "    features = sum(map(feature_function, pair))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with parallelization\n",
    "with Pool(6) as p:\n",
    "    t0 = time()\n",
    "    \n",
    "    features = p.map(get_features, pairs)\n",
    "    \n",
    "    t = time() - t0\n",
    "\n",
    "# Examine feature extraction result\n",
    "X = sparse.vstack(features)\n",
    "\n",
    "print('Extracted %i features from %i pairs' % X.shape[::-1])\n",
    "print('Time elapsed: %.2f minutes' % (t/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sparse.csr_matrix([1,0,0,9,0,2])\n",
    "s.shape[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 0.6460189731717099,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 0.717391365308206,\n",
       " 'gamma': 0.5220571710711481,\n",
       " 'learning_rate': 0.38181251415891027,\n",
       " 'max_delta_step': 6,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 0,\n",
       " 'missing': nan,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': -1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 47,\n",
       " 'reg_alpha': 0.001947442553211054,\n",
       " 'reg_lambda': 1.706103927414546,\n",
       " 'scale_pos_weight': 1.5261481450511425,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 0.6077746823368507,\n",
       " 'verbosity': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = joblib.load('best_model.pkl')\n",
    "clf.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
