{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Protein Interaction Network\n",
    "\n",
    "Machine learning classification task for predicting whether a protein pair interacts or not\n",
    "\n",
    "Protein pairs are generated from a test pathogen against human proteins contained in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import itertools\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from Bio import SearchIO\n",
    "\n",
    "from features import domain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "dir_in = os.path.join(parent_dir, 'data')\n",
    "dir_out = os.path.join(parent_dir, 'data', 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain pathogen protein domains\n",
    "\n",
    "Pathogen: *Streptococcus pneumoniae* strain D39 (**STRP2**)\n",
    "\n",
    "Protein sequence source: https://www.uniprot.org\n",
    "\n",
    "- `Organism [OS]: Streptococcus pneumoniae serotype 2 (strain D39 / NCTC 7466) [373153]`\n",
    "- `Sequence length: From 50`\n",
    "\n",
    "Extraction of Pfam domains: `hmmscan`\n",
    "- `hmmscan --tblout STRP2_pfam_hits --acc --noali -E 0.00001 --domE 0.00001 --cpu 7 ~/hmmer-3.2.1/pfam/Pfam-A.hmm STRP2_sequences.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pfam data from the training pathogens\n",
    "pfam_dict, pfam_set = joblib.load('pfam.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1643 STRP2 proteins for HP-PPI prediction\n"
     ]
    }
   ],
   "source": [
    "# Parse hmmscan result of STRP2\n",
    "f_in = os.path.join(dir_in, 'STRP2_pfam_hits')\n",
    "pathogen_prots = [] # store Uniprot accessions of STRP2\n",
    "\n",
    "for query in SearchIO.parse(f_in, 'hmmer3-tab'):\n",
    "    uniprot_id = query.id.split('|')[1]\n",
    "    domains = []\n",
    "    \n",
    "    # Read each domain hits in query\n",
    "    for hit in query.hits:\n",
    "        pfam_acc = hit.accession.split('.')[0] # Pfam accession of domain\n",
    "        \n",
    "        # Select only domains that exists in the training set\n",
    "        if pfam_acc in pfam_set: \n",
    "            domains.append(pfam_acc)\n",
    "        \n",
    "    # Add the pathogen protein to an existing domains dict\n",
    "    pfam_dict[uniprot_id] = domains \n",
    "    pathogen_prots.append(uniprot_id)\n",
    "\n",
    "# Print statistics\n",
    "print('Selected %i STRP2 proteins for HP-PPI prediction' % len(pathogen_prots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate candidate protein pairs and extract domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 3188 human proteins\n"
     ]
    }
   ],
   "source": [
    "# Extract human proteins from training dataset\n",
    "f_in = os.path.join(dir_in, 'positive_pairs.tsv')\n",
    "\n",
    "human_prots = list(set(pd.read_csv(f_in, sep='\\t')['Human_Uniprot_ID']))\n",
    "print('Obtained %i human proteins' % len(human_prots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5237884 protein pairs for prediction\n"
     ]
    }
   ],
   "source": [
    "# Generate protein pairs\n",
    "pairs = [pair for pair in itertools.product(pathogen_prots, human_prots)]\n",
    "print('Generated %i protein pairs for prediction' % len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4831/4831 [00:06<00:00, 722.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up feature extraction function\n",
    "feature_function = partial(domain_features,\n",
    "                           domain_dict=pfam_dict,\n",
    "                           domain_set=pfam_set)\n",
    "\n",
    "# Get features of each protein as dict to speed up\n",
    "# extraction from pairs\n",
    "all_prots = pathogen_prots + human_prots\n",
    "feat_dict = {prot: feature_function(prot) for prot in tqdm(all_prots)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP-PPI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 0.9991913298751062,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 0.7781926884888949,\n",
       " 'gamma': 1.8911208366536538,\n",
       " 'learning_rate': 0.2946712236771859,\n",
       " 'max_delta_step': 4,\n",
       " 'max_depth': 42,\n",
       " 'min_child_weight': 0,\n",
       " 'missing': nan,\n",
       " 'n_estimators': 2000,\n",
       " 'n_jobs': -1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 714903286,\n",
       " 'reg_alpha': 0.18374472468097258,\n",
       " 'reg_lambda': 2.6837199331440975,\n",
       " 'scale_pos_weight': 1.927967826686059,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 0.6817070748234033,\n",
       " 'verbosity': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained classifier\n",
    "clf = joblib.load('best_model.pkl')\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification task by batch\n",
    "n = int(1e4) # samples per batch\n",
    "batches = range(len(pairs) // n + 1)\n",
    "\n",
    "# Helper function for parallelization\n",
    "def get_predictions(i):\n",
    "    '''Predict interactions of all pairs in a batch'''\n",
    "    \n",
    "    lower = n * i\n",
    "    upper = n * (i + 1)\n",
    "    \n",
    "    # Extract features for all pairs in batch\n",
    "    X = []\n",
    "    for pair in pairs[lower:upper]:\n",
    "        features = sum(map(feat_dict.get, pair))\n",
    "        X.append(features)\n",
    "    \n",
    "    X = sparse.vstack(X)\n",
    "    \n",
    "    # Predict labels\n",
    "    return clf.predict(X, ntree_limit=clf.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification tasks complete (time elapsed: 7.23 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Batch prediction with parallelization\n",
    "with Pool(6) as p:\n",
    "    t0 = time()\n",
    "    \n",
    "    predictions = p.map(get_predictions, batches)\n",
    "    \n",
    "    t = time() - t0\n",
    "    print('Classification tasks complete (time elapsed: %.2f minutes)' % (t/60))\n",
    "\n",
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 4229503 interactions (80.75% of candidate pairs)\n"
     ]
    }
   ],
   "source": [
    "# Examine prediction result\n",
    "n_ppi = predictions.sum() # number of interactions reported\n",
    "print('Predicted %i interactions (%.2f%% of candidate pairs)' % (n_ppi, (n_ppi/len(pairs) * 100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
