{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Machine Learning Classifier for HP-PPI Prediction Task\n",
    "\n",
    "Classifier: XGBoost\n",
    "- performs well with sparse variables\n",
    "- needs no preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials, space_eval\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.simplefilter('ignore', category=(UndefinedMetricWarning, RuntimeWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "dir_in = os.path.join(parent_dir, 'data', 'features')\n",
    "dir_out = os.path.join(parent_dir, 'data', 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for combining datasets\n",
    "def get_dataset(pathogens):\n",
    "    '''Combine features and labels of different pathogen datasets'''\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Iterate through datasets\n",
    "    for pathogen in pathogens:\n",
    "        f_in = os.path.join(dir_in, '%s_features.pkl' % pathogen)\n",
    "        X_, y_ = joblib.load(f_in)\n",
    "        X.append(X_)\n",
    "        y.append(y_)\n",
    "    \n",
    "    # Combine features (X) and labels (y)\n",
    "    X = sparse.vstack(X)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Bayesian optimization with `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34955 samples with 4456 features\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "pathogens = ['Bacan', 'Yerpe', 'Fratu']\n",
    "X, y = get_dataset(pathogens)\n",
    "print('Loaded %i samples with %i features' % X.shape)\n",
    "\n",
    "# Get training data as DMatrix for xgboost\n",
    "pfam_acc = joblib.load('pfam.pkl')[1]\n",
    "dtrain = xgb.DMatrix(X, label=y, feature_names=pfam_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define eval function: F1-score\n",
    "def f1_eval(y, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred = [1. if y_i > 0.5 else 0. for y_i in y] # binarize output\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return 'f1', f1\n",
    "\n",
    "# Define objective function\n",
    "def objective(params):\n",
    "    cv_results = xgb.cv(params, dtrain, nfold=5, stratified=True,\n",
    "                        num_boost_round=1000, early_stopping_rounds=100,\n",
    "                        feval=f1_eval, maximize=True, seed=7)\n",
    "    \n",
    "    score = max(cv_results['test-f1-mean'])\n",
    "    \n",
    "    return {'loss': -score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [01:33<2:33:47, 93.21s/it, best loss: -0.4267834]"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1, 51)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 1.0),\n",
    "    'min_child_weight': hp.choice('min_child_weight', range(31)),\n",
    "    'max_delta_step': hp.choice('max_delta_step', range(21)),\n",
    "    'gamma': hp.uniform('gamma', 0, 10),\n",
    "    \n",
    "    'subsample': hp.uniform('subsample', 0.2, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.2, 1),\n",
    "    \n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.1, 10)\n",
    "}\n",
    "\n",
    "# Begin optimization\n",
    "trials = Trials()\n",
    "best = fmin(objective, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "# Obtain parameters of best model\n",
    "best_params = space_eval(param_space, best)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up classifier\n",
    "clf = xgb.XGBClassifier(**best_params, n_estimators=1000, n_jobs=-1)\n",
    "\n",
    "# Train with validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=7)\n",
    "_ = clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc',\n",
    "            early_stopping_rounds=20, verbose=1)\n",
    "\n",
    "# Dump model as pickle\n",
    "_ = joblib.dump(clf, 'best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on partitioned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test classifier on different datasets\n",
    "## Partition datasets by pathogen species\n",
    "i = 0 # track number of tests\n",
    "\n",
    "for p_test in pathogens:\n",
    "    p_train = [p for p in pathogens if p != p_test]\n",
    "    X_train, y_train = get_dataset(p_train)\n",
    "    \n",
    "    # Train classifier with validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.7,\n",
    "                                                      random_state=7)\n",
    "    \n",
    "    _ = clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc',\n",
    "                early_stopping_rounds=20, verbose=0)\n",
    "    \n",
    "    # Evaluate performance on test set\n",
    "    X_test, y_test = get_dataset([p_test])\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_proba = clf.predict_proba(X_test)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    print('Test %i'.center(70) % i)\n",
    "    print('Test Pathogen: %s\\n'.center(70) % p_test)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    # Plot metrics\n",
    "    _ = plot_confusion_matrix(y_test, y_pred)\n",
    "    _ = plot_roc(y_test, y_proba)\n",
    "    _ = plt.show()\n",
    "    \n",
    "    display(Markdown('<hr></hr>'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
